# Testing Patterns Guide

> *"Test early, test often, and test on the hardware that matters."* ðŸŽ¸

This guide covers testing patterns for MetalAudio, including PyTorch reference validation, CI resilience, and hardware-adaptive tolerances.

## Overview

MetalAudio tests face unique challenges:

- **GPU variability** â€” Different Metal drivers produce slightly different results
- **CI limitations** â€” CI runners may lack real GPUs or have software rendering
- **Numerical precision** â€” Float32 accumulation varies across hardware
- **Real-time constraints** â€” Audio code must be tested for allocation-free operation

---

## PyTorch Reference Validation

### Why Reference Testing?

Neural network layers must match PyTorch's behavior exactly (within floating-point tolerance). Reference testing:

1. **Guarantees correctness** â€” Validates against known-good PyTorch output
2. **Catches regressions** â€” Changes that break compatibility are detected
3. **Documents behavior** â€” Reference data serves as specification

### Reference Data Structure

```json
{
  "name": "linear_forward_batch4",
  "input": [0.1, 0.2, ...],
  "expectedOutput": [0.5, 0.6, ...],
  "inputShape": [4, 64],
  "outputShape": [4, 32],
  "parameters": {
    "weight": [[...], ...],
    "bias": [...]
  },
  "tolerance": 1e-5
}
```

### Using ReferenceTestUtils

```swift
import XCTest
@testable import MetalNN

class LinearReferenceTests: XCTestCase {

    func testLinearMatchesPyTorch() throws {
        // Load reference data (skips gracefully if not found)
        let (weights, testCases) = try ReferenceTestUtils.getLinearReferences()

        // Create layer with reference weights
        let linear = Linear(
            inFeatures: weights.inFeatures,
            outFeatures: weights.outFeatures
        )
        linear.loadWeights(weights.weight.flatMap { $0 }, bias: weights.bias)

        // Test each case
        for (name, input, expected) in testCases {
            let output = linear.forward(input.flatMap { $0 })

            ReferenceTestUtils.assertClose(
                output,
                expected.flatMap { $0 },
                rtol: 1e-5,
                atol: 1e-8,
                message: "Test case: \(name)"
            )
        }
    }
}
```

### Generating Reference Data

Reference data is generated by Python scripts that run PyTorch:

```bash
# Generate all reference data
python3 Scripts/generate_pytorch_references.py

# Output: Tests/MetalNNTests/Resources/pytorch_references.json
```

The script generates references for:

| Category | Test Cases |
|----------|------------|
| Activations | ReLU, Sigmoid, Tanh, GELU, Softmax (including edge cases) |
| Linear | Various batch sizes (1, 4, 16, 64) |
| Conv1D | Different kernel sizes, strides, padding |
| ConvTranspose1D | Upsampling configurations |
| LSTM | Single layer, multi-layer, bidirectional |
| GRU | Standard configuration |
| LayerNorm | Various normalized shapes |
| BatchNorm | Training and eval modes |
| Pooling | MaxPool, AvgPool, GlobalAvgPool |
| STFT | Using librosa for ground truth |
| Filters | Using scipy for frequency response |

### Adding New Reference Tests

1. **Add PyTorch code to generator:**

```python
# In Scripts/generate_pytorch_references.py
def generate_groupnorm_references():
    torch.manual_seed(42)

    # Create layer with known weights
    gn = nn.GroupNorm(num_groups=8, num_channels=48)

    # Generate test cases
    test_cases = {}
    for batch in [1, 2, 4]:
        x = torch.randn(batch, 48, 32)
        y = gn(x)
        test_cases[f"batch_{batch}"] = {
            "input": x.numpy().tolist(),
            "output": y.detach().numpy().tolist()
        }

    return {
        "params": {
            "weight": gn.weight.numpy().tolist(),
            "bias": gn.bias.numpy().tolist(),
            "num_groups": 8,
            "num_channels": 48,
            "eps": 1e-5
        },
        **test_cases
    }
```

2. **Add loader to ReferenceTestUtils:**

```swift
public static func getGroupNormReferences() throws -> (
    params: (weight: [Float], bias: [Float], numGroups: Int, numChannels: Int, eps: Float),
    testCases: [(name: String, input: [[[Float]]], output: [[[Float]]])]
) {
    let refs = try loadPyTorchReferences()
    guard let groupnorm = refs["groupnorm"] as? [String: Any] else {
        throw ReferenceError.invalidFormat("GroupNorm references not found")
    }
    // ... parse and return
}
```

3. **Write the test:**

```swift
func testGroupNormMatchesPyTorch() throws {
    let (params, testCases) = try ReferenceTestUtils.getGroupNormReferences()

    let groupNorm = try GroupNorm(
        device: device,
        numGroups: params.numGroups,
        numChannels: params.numChannels
    )
    try groupNorm.loadParameters(weight: params.weight, bias: params.bias)

    for (name, input, expected) in testCases {
        // ... test
    }
}
```

---

## CI Resilience Patterns

### TestEnvironment

The `TestEnvironment` enum detects CI and adapts test behavior:

```swift
enum TestEnvironment {
    // CI detection
    static var isCI: Bool { /* checks CI env vars */ }
    static var isLocal: Bool { !isCI }

    // GPU availability
    static var hasRealGPU: Bool { /* not software renderer */ }
    static var hasReliableGPU: Bool { hasRealGPU && isLocal && !isIOSSimulator }

    // Tolerance multipliers
    static var allocationToleranceMultiplier: Int { isCI ? 4 : 1 }
    static var numericalToleranceMultiplier: Float { isCI ? 10.0 : 1.0 }
    static var timingToleranceMultiplier: Double { isCI ? 5.0 : 1.0 }
}
```

### Skip Patterns

**Skip individual test:**

```swift
func testGPUSpecific() throws {
    try skipInCI("Requires local GPU environment")
    // ... test
}
```

**Skip test requiring reliable GPU:**

```swift
func testShaderCaching() throws {
    try skipWithoutReliableGPU("Shader binary archives not supported in CI")
    // ... test
}
```

**Skip entire test class:**

```swift
class GroupNormTests: XCTestCase {
    override func setUpWithError() throws {
        try XCTSkipIf(
            ProcessInfo.processInfo.environment["CI"] != nil,
            "Skipping GroupNorm tests on CI due to GPU driver issues"
        )
        device = try AudioDevice()
    }
}
```

### When to Skip vs. Adapt

| Situation | Action |
|-----------|--------|
| GPU produces NaN on CI | Skip (fundamental incompatibility) |
| Timing varies | Use `timingToleranceMultiplier` |
| Allocation higher on CI | Use `assertAllocationStable()` |
| Numerical precision varies | Use `assertNumericallyEqual()` |
| Shader caching unsupported | Skip with `skipWithoutReliableGPU()` |
| Reference data missing | Let `loadReference()` skip automatically |

### CI-Aware Assertions

```swift
// Allocation assertion with CI tolerance
assertAllocationStable(bytes, lessThan: 1000)
// In CI: threshold becomes 4000

// Numerical assertion with CI tolerance
assertNumericallyEqual(actual, expected, accuracy: 1e-5)
// In CI: tolerance becomes 1e-4
```

---

## Hardware-Adaptive Tolerances

### ToleranceProvider

Different GPU families have different numerical characteristics:

```swift
let tolerances = ToleranceProvider.shared.tolerances

// Use hardware-appropriate tolerance
XCTAssertEqual(output, expected, accuracy: tolerances.fftAccuracy)
```

### GPU Family Tolerances

| GPU Family | Typical FFT Tolerance | Notes |
|------------|----------------------|-------|
| Apple 9 (M4) | 1e-4 | Best precision |
| Apple 8 (M3) | 1e-4 | Excellent |
| Apple 7 (M1/M2) | 1e-3 | Good |
| Apple 6 and older | 1e-2 | Acceptable |
| Software renderer | 1e-1 | Very loose |

### Configuring Tolerances

```swift
// Global configuration
ToleranceConfiguration.shared.setTolerance(
    forOperation: .fft,
    gpuFamily: .apple7,
    tolerance: 1e-3
)

// Per-test override
func testHighPrecision() throws {
    let tolerance = ToleranceProvider.shared.tolerances.fftAccuracy
    let stricter = tolerance / 10  // 10x stricter for this test
    // ...
}
```

---

## Real-Time Safety Testing

### Allocation-Free Verification

Audio render callbacks must not allocate. Test this:

```swift
func testPredictIsAllocationFree() throws {
    let inference = try BNNSInference(modelPath: modelURL)
    var input = [Float](repeating: 0, count: 64)
    var output = [Float](repeating: 0, count: 32)

    // Warm up (may allocate)
    inference.predict(input: &input, output: &output)

    // Measure allocations during actual use
    let allocations = measureAllocations {
        for _ in 0..<100 {
            inference.predict(input: &input, output: &output)
        }
    }

    assertAllocationStable(allocations, lessThan: 100)  // < 100 bytes allowed
}
```

### Thread Safety Testing

```swift
func testConcurrentAccess() throws {
    let device = try AudioDevice()

    // Test concurrent reads (should be safe)
    DispatchQueue.concurrentPerform(iterations: 100) { _ in
        _ = device.hardwareProfile
    }

    // For thread-safe classes, test concurrent operations
    let inference = try BNNSInference(modelPath: modelURL)
    DispatchQueue.concurrentPerform(iterations: 100) { i in
        var input = [Float](repeating: Float(i), count: 64)
        var output = [Float](repeating: 0, count: 32)
        inference.predict(input: &input, output: &output)
    }
}
```

---

## NaN/Inf Detection

GPU operations can produce numerical instability. Always validate:

```swift
func testNoNaNInOutput() throws {
    let result = try layer.forward(input: tensor, encoder: encoder)
    let output = result.toArray()

    XCTAssertFalse(output.contains { $0.isNaN }, "Output contains NaN")
    XCTAssertFalse(output.contains { $0.isInfinite }, "Output contains Inf")

    // Also check for reasonable magnitude
    let maxMagnitude = output.map { abs($0) }.max() ?? 0
    XCTAssertLessThan(maxMagnitude, 1e6, "Output has unreasonable magnitude")
}
```

---

## Test Organization

### Directory Structure

```text
Tests/
â”œâ”€â”€ MetalAudioKitTests/     # Core GPU, tensor, device tests
â”‚   â”œâ”€â”€ TestUtilities.swift # Shared utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ MetalDSPTests/          # FFT, convolution, filter tests
â”‚   â”œâ”€â”€ TestUtilities.swift # Shared utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ MetalNNTests/           # Neural network layer tests
â”‚   â”œâ”€â”€ TestUtilities.swift # Shared utilities
â”‚   â”œâ”€â”€ ReferenceTestUtils.swift # PyTorch reference loading
â”‚   â””â”€â”€ Resources/          # Reference data, CoreML models
â””â”€â”€ CLAUDE.md               # Test documentation
```

### Naming Conventions

| Pattern | Use Case |
|---------|----------|
| `test<Feature>_<Scenario>` | Unit tests |
| `test<Layer>MatchesPyTorch` | Reference validation |
| `test<Operation>IsAllocationFree` | Real-time safety |
| `test<Class>ThreadSafety` | Concurrency |

---

## Debugging Test Failures

### Reference Mismatch

```swift
// Get detailed error info
let (maxErr, meanErr, rmsErr) = ReferenceTestUtils.relativeError(actual, expected)
print("Max error: \(maxErr), Mean: \(meanErr), RMS: \(rmsErr)")

// Find first differing element
for i in 0..<actual.count {
    if abs(actual[i] - expected[i]) > tolerance {
        print("First diff at \(i): actual=\(actual[i]), expected=\(expected[i])")
        break
    }
}
```

### GPU Driver Issues

```swift
// Check GPU info
let device = MTLCreateSystemDefaultDevice()!
print("GPU: \(device.name)")
print("Family: \(device.supportsFamily(.apple7) ? "Apple 7+" : "older")")

// Try different algorithm
try groupNorm.setAlgorithm(.welford)  // More stable than .standard
```

### CI-Specific Failures

```bash
# Run locally with CI simulation
CI=true swift test --filter 'GroupNormTests'

# Check what CI sees
CI=true swift test --filter 'TestEnvironment'
```

---

## Best Practices

1. **Always use reference data** â€” Don't hardcode expected values
2. **Skip gracefully** â€” Use `XCTSkip` not `XCTFail` for environment issues
3. **Test edge cases** â€” Near-zero, near-max, denormals
4. **Validate numerics** â€” Check for NaN/Inf in all GPU outputs
5. **Document skip reasons** â€” Future maintainers need context
6. **Regenerate references** â€” When PyTorch version changes

---

*Test like your audio depends on it â€” because it does.* ðŸ¤˜
